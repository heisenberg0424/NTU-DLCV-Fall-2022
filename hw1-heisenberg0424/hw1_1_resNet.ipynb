{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afe0f73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd9d01c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hw1_1 import *\n",
    "from torch.optim import Adam,SGD\n",
    "from torch.autograd import Variable\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b14566df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: lr = 0.000030, fc.bias\n",
      "1: lr = 0.000030, fc.weight\n",
      "2: lr = 0.000030, layer4.2.bn3.bias\n",
      "3: lr = 0.000030, layer4.2.bn3.weight\n",
      "4: lr = 0.000030, layer4.2.conv3.weight\n",
      "5: lr = 0.000030, layer4.2.bn2.bias\n",
      "6: lr = 0.000030, layer4.2.bn2.weight\n",
      "7: lr = 0.000030, layer4.2.conv2.weight\n",
      "8: lr = 0.000030, layer4.2.bn1.bias\n",
      "9: lr = 0.000030, layer4.2.bn1.weight\n",
      "10: lr = 0.000030, layer4.2.conv1.weight\n",
      "11: lr = 0.000030, layer4.1.bn3.bias\n",
      "12: lr = 0.000030, layer4.1.bn3.weight\n",
      "13: lr = 0.000030, layer4.1.conv3.weight\n",
      "14: lr = 0.000030, layer4.1.bn2.bias\n",
      "15: lr = 0.000030, layer4.1.bn2.weight\n",
      "16: lr = 0.000030, layer4.1.conv2.weight\n",
      "17: lr = 0.000030, layer4.1.bn1.bias\n",
      "18: lr = 0.000030, layer4.1.bn1.weight\n",
      "19: lr = 0.000030, layer4.1.conv1.weight\n",
      "20: lr = 0.000030, layer4.0.downsample.1.bias\n",
      "21: lr = 0.000030, layer4.0.downsample.1.weight\n",
      "22: lr = 0.000030, layer4.0.downsample.0.weight\n",
      "23: lr = 0.000030, layer4.0.bn3.bias\n",
      "24: lr = 0.000030, layer4.0.bn3.weight\n",
      "25: lr = 0.000030, layer4.0.conv3.weight\n",
      "26: lr = 0.000030, layer4.0.bn2.bias\n",
      "27: lr = 0.000030, layer4.0.bn2.weight\n",
      "28: lr = 0.000030, layer4.0.conv2.weight\n",
      "29: lr = 0.000030, layer4.0.bn1.bias\n",
      "30: lr = 0.000030, layer4.0.bn1.weight\n",
      "31: lr = 0.000030, layer4.0.conv1.weight\n",
      "32: lr = 0.000021, layer3.5.bn3.bias\n",
      "33: lr = 0.000021, layer3.5.bn3.weight\n",
      "34: lr = 0.000021, layer3.5.conv3.weight\n",
      "35: lr = 0.000021, layer3.5.bn2.bias\n",
      "36: lr = 0.000021, layer3.5.bn2.weight\n",
      "37: lr = 0.000021, layer3.5.conv2.weight\n",
      "38: lr = 0.000021, layer3.5.bn1.bias\n",
      "39: lr = 0.000021, layer3.5.bn1.weight\n",
      "40: lr = 0.000021, layer3.5.conv1.weight\n",
      "41: lr = 0.000021, layer3.4.bn3.bias\n",
      "42: lr = 0.000021, layer3.4.bn3.weight\n",
      "43: lr = 0.000021, layer3.4.conv3.weight\n",
      "44: lr = 0.000021, layer3.4.bn2.bias\n",
      "45: lr = 0.000021, layer3.4.bn2.weight\n",
      "46: lr = 0.000021, layer3.4.conv2.weight\n",
      "47: lr = 0.000021, layer3.4.bn1.bias\n",
      "48: lr = 0.000021, layer3.4.bn1.weight\n",
      "49: lr = 0.000021, layer3.4.conv1.weight\n",
      "50: lr = 0.000021, layer3.3.bn3.bias\n",
      "51: lr = 0.000021, layer3.3.bn3.weight\n",
      "52: lr = 0.000021, layer3.3.conv3.weight\n",
      "53: lr = 0.000021, layer3.3.bn2.bias\n",
      "54: lr = 0.000021, layer3.3.bn2.weight\n",
      "55: lr = 0.000021, layer3.3.conv2.weight\n",
      "56: lr = 0.000021, layer3.3.bn1.bias\n",
      "57: lr = 0.000021, layer3.3.bn1.weight\n",
      "58: lr = 0.000021, layer3.3.conv1.weight\n",
      "59: lr = 0.000021, layer3.2.bn3.bias\n",
      "60: lr = 0.000021, layer3.2.bn3.weight\n",
      "61: lr = 0.000021, layer3.2.conv3.weight\n",
      "62: lr = 0.000021, layer3.2.bn2.bias\n",
      "63: lr = 0.000021, layer3.2.bn2.weight\n",
      "64: lr = 0.000021, layer3.2.conv2.weight\n",
      "65: lr = 0.000021, layer3.2.bn1.bias\n",
      "66: lr = 0.000021, layer3.2.bn1.weight\n",
      "67: lr = 0.000021, layer3.2.conv1.weight\n",
      "68: lr = 0.000021, layer3.1.bn3.bias\n",
      "69: lr = 0.000021, layer3.1.bn3.weight\n",
      "70: lr = 0.000021, layer3.1.conv3.weight\n",
      "71: lr = 0.000021, layer3.1.bn2.bias\n",
      "72: lr = 0.000021, layer3.1.bn2.weight\n",
      "73: lr = 0.000021, layer3.1.conv2.weight\n",
      "74: lr = 0.000021, layer3.1.bn1.bias\n",
      "75: lr = 0.000021, layer3.1.bn1.weight\n",
      "76: lr = 0.000021, layer3.1.conv1.weight\n",
      "77: lr = 0.000021, layer3.0.downsample.1.bias\n",
      "78: lr = 0.000021, layer3.0.downsample.1.weight\n",
      "79: lr = 0.000021, layer3.0.downsample.0.weight\n",
      "80: lr = 0.000021, layer3.0.bn3.bias\n",
      "81: lr = 0.000021, layer3.0.bn3.weight\n",
      "82: lr = 0.000021, layer3.0.conv3.weight\n",
      "83: lr = 0.000021, layer3.0.bn2.bias\n",
      "84: lr = 0.000021, layer3.0.bn2.weight\n",
      "85: lr = 0.000021, layer3.0.conv2.weight\n",
      "86: lr = 0.000021, layer3.0.bn1.bias\n",
      "87: lr = 0.000021, layer3.0.bn1.weight\n",
      "88: lr = 0.000021, layer3.0.conv1.weight\n",
      "89: lr = 0.000015, layer2.3.bn3.bias\n",
      "90: lr = 0.000015, layer2.3.bn3.weight\n",
      "91: lr = 0.000015, layer2.3.conv3.weight\n",
      "92: lr = 0.000015, layer2.3.bn2.bias\n",
      "93: lr = 0.000015, layer2.3.bn2.weight\n",
      "94: lr = 0.000015, layer2.3.conv2.weight\n",
      "95: lr = 0.000015, layer2.3.bn1.bias\n",
      "96: lr = 0.000015, layer2.3.bn1.weight\n",
      "97: lr = 0.000015, layer2.3.conv1.weight\n",
      "98: lr = 0.000015, layer2.2.bn3.bias\n",
      "99: lr = 0.000015, layer2.2.bn3.weight\n",
      "100: lr = 0.000015, layer2.2.conv3.weight\n",
      "101: lr = 0.000015, layer2.2.bn2.bias\n",
      "102: lr = 0.000015, layer2.2.bn2.weight\n",
      "103: lr = 0.000015, layer2.2.conv2.weight\n",
      "104: lr = 0.000015, layer2.2.bn1.bias\n",
      "105: lr = 0.000015, layer2.2.bn1.weight\n",
      "106: lr = 0.000015, layer2.2.conv1.weight\n",
      "107: lr = 0.000015, layer2.1.bn3.bias\n",
      "108: lr = 0.000015, layer2.1.bn3.weight\n",
      "109: lr = 0.000015, layer2.1.conv3.weight\n",
      "110: lr = 0.000015, layer2.1.bn2.bias\n",
      "111: lr = 0.000015, layer2.1.bn2.weight\n",
      "112: lr = 0.000015, layer2.1.conv2.weight\n",
      "113: lr = 0.000015, layer2.1.bn1.bias\n",
      "114: lr = 0.000015, layer2.1.bn1.weight\n",
      "115: lr = 0.000015, layer2.1.conv1.weight\n",
      "116: lr = 0.000015, layer2.0.downsample.1.bias\n",
      "117: lr = 0.000015, layer2.0.downsample.1.weight\n",
      "118: lr = 0.000015, layer2.0.downsample.0.weight\n",
      "119: lr = 0.000015, layer2.0.bn3.bias\n",
      "120: lr = 0.000015, layer2.0.bn3.weight\n",
      "121: lr = 0.000015, layer2.0.conv3.weight\n",
      "122: lr = 0.000015, layer2.0.bn2.bias\n",
      "123: lr = 0.000015, layer2.0.bn2.weight\n",
      "124: lr = 0.000015, layer2.0.conv2.weight\n",
      "125: lr = 0.000015, layer2.0.bn1.bias\n",
      "126: lr = 0.000015, layer2.0.bn1.weight\n",
      "127: lr = 0.000015, layer2.0.conv1.weight\n",
      "128: lr = 0.000010, layer1.2.bn3.bias\n",
      "129: lr = 0.000010, layer1.2.bn3.weight\n",
      "130: lr = 0.000010, layer1.2.conv3.weight\n",
      "131: lr = 0.000010, layer1.2.bn2.bias\n",
      "132: lr = 0.000010, layer1.2.bn2.weight\n",
      "133: lr = 0.000010, layer1.2.conv2.weight\n",
      "134: lr = 0.000010, layer1.2.bn1.bias\n",
      "135: lr = 0.000010, layer1.2.bn1.weight\n",
      "136: lr = 0.000010, layer1.2.conv1.weight\n",
      "137: lr = 0.000010, layer1.1.bn3.bias\n",
      "138: lr = 0.000010, layer1.1.bn3.weight\n",
      "139: lr = 0.000010, layer1.1.conv3.weight\n",
      "140: lr = 0.000010, layer1.1.bn2.bias\n",
      "141: lr = 0.000010, layer1.1.bn2.weight\n",
      "142: lr = 0.000010, layer1.1.conv2.weight\n",
      "143: lr = 0.000010, layer1.1.bn1.bias\n",
      "144: lr = 0.000010, layer1.1.bn1.weight\n",
      "145: lr = 0.000010, layer1.1.conv1.weight\n",
      "146: lr = 0.000010, layer1.0.downsample.1.bias\n",
      "147: lr = 0.000010, layer1.0.downsample.1.weight\n",
      "148: lr = 0.000010, layer1.0.downsample.0.weight\n",
      "149: lr = 0.000010, layer1.0.bn3.bias\n",
      "150: lr = 0.000010, layer1.0.bn3.weight\n",
      "151: lr = 0.000010, layer1.0.conv3.weight\n",
      "152: lr = 0.000010, layer1.0.bn2.bias\n",
      "153: lr = 0.000010, layer1.0.bn2.weight\n",
      "154: lr = 0.000010, layer1.0.conv2.weight\n",
      "155: lr = 0.000010, layer1.0.bn1.bias\n",
      "156: lr = 0.000010, layer1.0.bn1.weight\n",
      "157: lr = 0.000010, layer1.0.conv1.weight\n",
      "158: lr = 0.000007, bn1.bias\n",
      "159: lr = 0.000007, bn1.weight\n",
      "160: lr = 0.000005, conv1.weight\n"
     ]
    }
   ],
   "source": [
    "model,trans = myResnet(finetune=True)\n",
    "layer_names = []\n",
    "for idx, (name, param) in enumerate(model.named_parameters()):\n",
    "    layer_names.append(name)\n",
    "\n",
    "layer_names.reverse()\n",
    "lr      = 3e-5\n",
    "lr_mult = 0.7\n",
    "\n",
    "# placeholder\n",
    "parameters      = []\n",
    "prev_group_name = layer_names[2].split('.')[0]\n",
    "\n",
    "# store params & learning rates\n",
    "for idx, name in enumerate(layer_names):\n",
    "    if idx in[0,1]:\n",
    "        parameters += [{'params': [p for n, p in model.named_parameters() if n == name and p.requires_grad],\n",
    "                    'lr':     0.001}]\n",
    "        print(f'{idx}: lr = {lr:.6f}, {name}')\n",
    "        \n",
    "        continue\n",
    "    # parameter group name\n",
    "    cur_group_name = name.split('.')[0]\n",
    "    \n",
    "    # update learning rate\n",
    "    if cur_group_name != prev_group_name:\n",
    "        lr *= lr_mult\n",
    "    prev_group_name = cur_group_name\n",
    "    \n",
    "    # display info\n",
    "    print(f'{idx}: lr = {lr:.6f}, {name}')\n",
    "    \n",
    "    # append layer parameters\n",
    "    parameters += [{'params': [p for n, p in model.named_parameters() if n == name and p.requires_grad],\n",
    "                    'lr':     lr}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96ca949b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = loadData('hw1_data/hw1_data/p1_data/train_50',trans,data_aug=True)\n",
    "val_loader = loadData('hw1_data/hw1_data/p1_data/val_50',trans)\n",
    "\n",
    "\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a303ca45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveModel():\n",
    "    path = \"./hw1_1_myResnet.pth\"\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "def valAccuracy():\n",
    "    model.eval().cuda()\n",
    "    accuracy = 0.0\n",
    "    total = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            images, labels = data\n",
    "            outputs = model(images.cuda())\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            accuracy += (predicted == labels.cuda()).sum().item()\n",
    "    \n",
    "    accuracy = (100 * accuracy / total)\n",
    "    return(accuracy)\n",
    "\n",
    "\n",
    "def train(num_epochs):\n",
    "    \n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"The model will be running on\", device, \"device\")\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "        start = datetime.datetime.now()\n",
    "        for i, (images, labels) in enumerate(train_loader, 0):\n",
    "            \n",
    "            # get the inputs\n",
    "            images = Variable(images.to(device))\n",
    "            labels = Variable(labels.to(device))\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # predict classes using images from the training set\n",
    "            outputs = model(images)\n",
    "            # compute the loss based on model output and real labels\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            # backpropagate the loss\n",
    "            loss.backward()\n",
    "            # adjust parameters based on the calculated gradients\n",
    "            optimizer.step()\n",
    "\n",
    "        end = datetime.datetime.now()\n",
    "        # Compute and print the average accuracy fo this epoch when tested over all test images\n",
    "        accuracy = valAccuracy()\n",
    "        print('For epoch', epoch+1,': test accuracy:{:.4f}%, loss:{:.4f}, time:{}'.format(accuracy,loss.item(),end-start))\n",
    "        \n",
    "        # we want to save the model if the accuracy is the best\n",
    "        if accuracy > best_accuracy:\n",
    "            saveModel()\n",
    "            best_accuracy = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ede904f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model will be running on cuda:0 device\n",
      "For epoch 1 : test accuracy:81.8800%, loss:0.5380, time:0:04:51.513415\n",
      "For epoch 2 : test accuracy:83.4800%, loss:0.3306, time:0:04:38.019422\n",
      "For epoch 3 : test accuracy:83.4400%, loss:0.3063, time:0:04:38.436704\n",
      "For epoch 4 : test accuracy:84.3600%, loss:0.0403, time:0:04:39.273481\n",
      "For epoch 5 : test accuracy:83.9600%, loss:0.0002, time:0:04:38.219280\n",
      "For epoch 6 : test accuracy:85.3600%, loss:0.0011, time:0:04:38.172458\n",
      "For epoch 7 : test accuracy:84.6800%, loss:0.0126, time:0:04:38.251480\n",
      "For epoch 8 : test accuracy:83.4400%, loss:0.0054, time:0:04:40.007428\n",
      "For epoch 9 : test accuracy:85.0800%, loss:0.0075, time:0:04:39.761769\n",
      "For epoch 10 : test accuracy:85.5200%, loss:0.0552, time:0:04:39.180326\n",
      "For epoch 11 : test accuracy:84.0000%, loss:0.0011, time:0:04:38.352262\n",
      "For epoch 12 : test accuracy:82.6400%, loss:0.0003, time:0:04:38.059576\n",
      "For epoch 13 : test accuracy:84.4800%, loss:0.0001, time:0:04:39.468355\n",
      "For epoch 14 : test accuracy:84.8800%, loss:0.1322, time:0:04:40.438391\n",
      "For epoch 15 : test accuracy:85.5600%, loss:0.0004, time:0:04:41.232118\n",
      "For epoch 16 : test accuracy:85.1200%, loss:0.0001, time:0:04:39.296946\n",
      "For epoch 17 : test accuracy:84.8000%, loss:0.0124, time:0:04:39.330866\n",
      "For epoch 18 : test accuracy:84.4800%, loss:0.0000, time:0:04:41.211820\n",
      "For epoch 19 : test accuracy:85.2000%, loss:0.0008, time:0:05:06.115440\n",
      "For epoch 20 : test accuracy:85.3200%, loss:0.0004, time:0:05:12.182074\n",
      "For epoch 21 : test accuracy:84.3600%, loss:0.0023, time:0:04:41.630876\n",
      "For epoch 22 : test accuracy:85.4400%, loss:0.0010, time:0:04:41.795861\n",
      "For epoch 23 : test accuracy:84.8800%, loss:0.0674, time:0:04:39.436125\n",
      "For epoch 24 : test accuracy:85.1600%, loss:0.0052, time:0:04:38.774060\n",
      "For epoch 25 : test accuracy:84.7200%, loss:0.0020, time:0:04:40.376051\n",
      "For epoch 26 : test accuracy:86.1600%, loss:0.0001, time:0:04:40.662374\n",
      "For epoch 27 : test accuracy:82.9600%, loss:0.0001, time:0:04:41.224121\n",
      "For epoch 28 : test accuracy:86.0400%, loss:0.0006, time:0:04:39.692698\n",
      "For epoch 29 : test accuracy:85.3600%, loss:0.0001, time:0:04:39.233928\n",
      "For epoch 30 : test accuracy:84.9600%, loss:0.0008, time:0:04:39.561741\n",
      "For epoch 31 : test accuracy:86.0000%, loss:0.0004, time:0:04:38.549694\n",
      "For epoch 32 : test accuracy:85.4000%, loss:0.3157, time:0:04:38.910068\n",
      "For epoch 33 : test accuracy:85.9600%, loss:0.0000, time:0:04:39.236240\n",
      "For epoch 34 : test accuracy:85.3600%, loss:0.0000, time:0:04:41.117831\n",
      "For epoch 35 : test accuracy:85.6800%, loss:0.0000, time:0:04:39.007979\n",
      "For epoch 36 : test accuracy:84.6400%, loss:0.1164, time:0:04:38.546411\n",
      "For epoch 37 : test accuracy:85.9600%, loss:0.0000, time:0:04:39.336827\n",
      "For epoch 38 : test accuracy:84.0400%, loss:0.0005, time:0:04:39.372266\n",
      "For epoch 39 : test accuracy:84.2400%, loss:0.0011, time:0:04:38.949277\n",
      "For epoch 40 : test accuracy:84.0800%, loss:0.0692, time:0:04:39.382412\n",
      "For epoch 41 : test accuracy:85.8800%, loss:0.0001, time:0:04:39.638138\n",
      "For epoch 42 : test accuracy:84.5600%, loss:0.0012, time:0:04:39.130780\n",
      "For epoch 43 : test accuracy:84.9600%, loss:0.0000, time:0:04:39.600594\n",
      "For epoch 44 : test accuracy:84.9200%, loss:0.0000, time:0:04:39.784390\n",
      "For epoch 45 : test accuracy:85.6000%, loss:0.0000, time:0:04:39.757686\n",
      "For epoch 46 : test accuracy:83.3600%, loss:0.0000, time:0:04:39.002627\n",
      "For epoch 47 : test accuracy:85.3600%, loss:0.0001, time:0:04:40.056129\n",
      "For epoch 48 : test accuracy:86.4000%, loss:0.0000, time:0:04:39.433087\n",
      "For epoch 49 : test accuracy:85.0800%, loss:0.0130, time:0:04:39.058816\n",
      "For epoch 50 : test accuracy:85.9600%, loss:0.0000, time:0:04:39.885025\n"
     ]
    }
   ],
   "source": [
    "train(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51696dcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.8 (DLCVenv)",
   "language": "python",
   "name": "dlcvenc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
